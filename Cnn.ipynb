{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # to handle matrix and data operation\n",
    "import pandas as pd # to read csv and handle dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_train = np.load('data/fashion_train.npy', mmap_mode='r')\n",
    "data_test = np.load('data/fashion_test.npy', mmap_mode='r')\n",
    "\n",
    "X_train = data_train[:, :-1]\n",
    "y_train = data_train[:, -1]\n",
    "\n",
    "X_test = data_test[:, :-1]\n",
    "y_test = data_test[:, -1]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train).long()\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long) \n",
    "\n",
    "# Reshape X_train and X_test for convolutional input (assuming 28x28 images)\n",
    "X_train_tensor = X_train_tensor.view(-1, 1, 28, 28)  # Adjust dimensions based on your actual image size\n",
    "X_test_tensor = X_test_tensor.view(-1, 1, 28, 28)  # Adjust dimensions based on your actual image size\n",
    "\n",
    "\n",
    "# Create TensorDataset\n",
    "data = TensorDataset(X_train_tensor, y_train_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1, 28, 28])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(data, [4000,6000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, **INPUT):\n",
    "\n",
    "        # Inherit from nn module\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Define activation function\n",
    "        self.af = INPUT.get('af')\n",
    "\n",
    "        # Define drop ratio\n",
    "        self.dpr = INPUT.get('dpr')\n",
    "\n",
    "        # Define neural network architecture\n",
    "        self.nn = nn.Sequential(\n",
    "                # C1 6@28x28\n",
    "                nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(6),\n",
    "                self.af(),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                \n",
    "                # C2: 16@10x10\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(16),\n",
    "                self.af(),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                \n",
    "                # Apply flattening on the output\n",
    "                nn.Flatten(),\n",
    "                \n",
    "                # Dense part\n",
    "                # L1\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Linear(16 * 5 * 5, 120),\n",
    "                nn.BatchNorm1d(120),\n",
    "                self.af(),\n",
    "                \n",
    "                # L2\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Linear(120, 84),\n",
    "                nn.BatchNorm1d(84),\n",
    "                self.af(),\n",
    "                \n",
    "                # L3\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Linear(84, 10))\n",
    "        \n",
    "        # Define batch size\n",
    "        self.batch_size = INPUT.get('batch_size')\n",
    "\n",
    "        # Define datasets\n",
    "        self.training_data = DataLoader(\n",
    "            INPUT.get('trd'), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "        self.validation_data = DataLoader(\n",
    "            INPUT.get('vd'), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "        self.test_data = DataLoader(\n",
    "            INPUT.get('ted'), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss_fn = INPUT.get('loss_fn')\n",
    "\n",
    "        # Define learning rate\n",
    "        self.lr = INPUT.get('lr')\n",
    "\n",
    "        # Define numper of epochs\n",
    "        self.epochs = INPUT.get('epochs')\n",
    "\n",
    "        # Define optimizer\n",
    "        self.optimizer = INPUT.get('optim')(self.parameters(), lr=self.lr)\n",
    "\n",
    "        # Save training progress\n",
    "        self.loss_history = []\n",
    "        self.acc_history = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.nn(x)\n",
    "        return logits\n",
    "    \n",
    "    def train_loop(self):\n",
    "        \n",
    "        size = len(self.training_data.dataset)\n",
    "        for batch, (X, y) in enumerate(self.training_data):\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred = self.forward(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    def val_loop(self):\n",
    "        size = len(self.validation_data.dataset)\n",
    "        num_batches = len(self.validation_data)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.validation_data:\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "\n",
    "        print(\n",
    "            f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        # Save it to history\n",
    "        self.acc_history.append(correct)\n",
    "        self.loss_history.append(test_loss)\n",
    "\n",
    "    def visualize(self):\n",
    "        x = [i for i in range(self.epochs)]\n",
    "        y1 = self.acc_history\n",
    "        y2 = self.loss_history\n",
    "        plt.plot(x, y1)\n",
    "        plt.plot(x, y2)\n",
    "        plt.show()\n",
    "\n",
    "    def fit(self):\n",
    "        for t in range(self.epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            self.train_loop()\n",
    "            self.val_loop()\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        return softmax(logits).argmax(1)\n",
    "\n",
    "    def test(self):\n",
    "        # Get data\n",
    "        X, y = next(iter(self.test_data))\n",
    "\n",
    "        # Predict values\n",
    "        y_hat = self.predict(X)\n",
    "\n",
    "        print(\"Accuracy score for test data\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"Acc: {accuracy_score(y, y_hat)*100} %\")\n",
    "        print()\n",
    "        print(\"Confusion matrix for test data\")\n",
    "        print(\"-\"*60)\n",
    "        print(confusion_matrix(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.466106  [    0/ 4000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.677281 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.624029  [    0/ 4000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.573078 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.628554  [    0/ 4000]\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.527018 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.534920  [    0/ 4000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.472094 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.467093  [    0/ 4000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.418269 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "INPUT = {\n",
    "    'batch_size': 100,\n",
    "    'trd': train_set,\n",
    "    'vd': val_set ,\n",
    "    'ted': X_test_tensor,\n",
    "    'loss_fn': nn.CrossEntropyLoss(),\n",
    "    'lr': 1e-1,\n",
    "    'epochs': 5,\n",
    "    'af': nn.Sigmoid,\n",
    "    'optim': torch.optim.Adam,\n",
    "    'dpr': 1e-3\n",
    "\n",
    "}\n",
    "model = CNN(**INPUT)\n",
    "\n",
    "# Train model\n",
    "model.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
