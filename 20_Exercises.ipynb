{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import tensor\n",
    "import torch.optim as optim\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network - Convolutional NeuralNetworks\n",
    "\n",
    "### Summary\n",
    "The exercises here aim for some understanding and some hands-on exeprience with CNNs. Before these exercises, I also suggest that you have a look at the exampleFilters.ipynb to see some simple filters that detect vertical and horizotal edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Consider a CNN that takes in 32 × 32 grayscale images and has a single convolution layer with three 5 × 5 convolution filters (without padding).\n",
    "- What is the size of the feature map? (feature map is the output of one filter applied to the previous layer)\n",
    "- With what size of padding we will end up to a feature map with the same size as the input (this is sometimes called \"same padding\")?\n",
    "- How many parameters are in this model?\n",
    "- Explain how this model can be thought of as an ordinary feedforward neural network with the individual pixels as inputs? are there any kind of constraints on the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions\n",
    "- As an input we takee 32 x 32 grayscale image\n",
    "- We run the image through thre 5x5 filters\n",
    "- We will use no padding around the image\n",
    "- **The stride length is 1**\n",
    "\n",
    "Every time we run the imagee through a filter its dimensions will shrink by 4.\n",
    "\n",
    "To compute the size of the outputted channel, we use that\n",
    "- input dimension is $N=32$\n",
    "- filter dimension is $F=5$\n",
    "- we use no padding, therefore the input dimension $N$ will shrink by $F-1$.\n",
    "\n",
    "Therefore the output dimension will be\n",
    "\n",
    "$$\n",
    "O=N-(F-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "O=32-(5-1)=28\n",
    "$$\n",
    "\n",
    "The output of the feature map is 28 x 28.\n",
    "\n",
    "If we want to end up with an output of the same dimension as the input, then we need to add a padding of 2 to each side of the image. \n",
    "\n",
    "We can compute the desired dimension of the input, $N$, as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "O &= N-(F-1)\n",
    "N &= O+(F-1)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Given the desired output size $O$ and the filter size $F$, we can compute the needed input size $N$.\n",
    "\n",
    "$$\n",
    "N = 32 + (5-1)=36\n",
    "$$\n",
    "\n",
    "We need to add 4 pixels to each row and column in the input. Meaning, that on each side of the input image, we need to add 2 pixels. Using the padded image, we can obtain output image with the same dimension.\n",
    "\n",
    "Each filter has 25 parameters. And since there is three filters, this means that there we have 75 parameters in total.\n",
    "\n",
    "\n",
    "You can think of it as a feedforward neural nework, where each pixel in a convolutional layer is a neuron, and the filters are set of weights. Each neuron, takes 25 pixels as input (either from the input layer of from the previous convolution layer). But in opposition to a FFNN, all neurons in the same layer uses the same weights. This significantly reduces the number of parameters. \n",
    "\n",
    "Last but not the least, why do we actually need convolutional layer? Well, the input image has 1024 pixels, if we would just feed these to our FFNN, we would lose a lot of information, especially about the order of the pixels. Therefore, the core idea behind convolutional layer is to have model learn some patterns from the image and THEN feed these patterns as an input to our FFNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps (i.e., channels), the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "- What is the total number of parameters in the CNN? If we are using 32-bit floats for every parameter, at least how much RAM will this network require when making a prediction for a single instance?\n",
    "- What about when training on a mini-batch of 50 images?\n",
    "- Why would you want to add a max pooling layer rather than a convolutional layer (with the same stride)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link for ouput dimensions: https://kvirajdatt.medium.com/calculating-output-dimensions-in-a-cnn-for-convolution-and-pooling-layers-with-keras-682960c73870"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same padding and output size (Convolution layer)\n",
    "We can compute the convolution output dimension as\n",
    "\n",
    "$$\n",
    "O = [(I-F+2\\cdot P)/S]+1 \\times D\n",
    "$$\n",
    "\n",
    "where I is the input dimensions of the image ($i \\times i$), F is the size of the filter/kernel ($f \\times f$), S is the strides, P is the padding and D is the depth (number of feature maps).\n",
    "\n",
    "If your are using same padding with stride > 1, P will be the minimum number to make $(I-F+2\\cdot P)$ divisible by S.\n",
    "\n",
    "Same padding means that \n",
    "\n",
    "$$\n",
    "O=\\left\\lceil \\frac{I}{S} \\right\\rceil\n",
    "$$\n",
    "\n",
    "The generic output formula is\n",
    "\n",
    "$$\n",
    "O = \\left\\lfloor (I-F+2\\cdot P)/2 + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "If you solve for $P$, you get that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_{min} &= (O-1)S-I+F \\\\\n",
    "p_{max} &= O\\cdot S-I+F-1\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output size (Max pooling layer)\n",
    "For a pooling layer, one can specify only the filter/kernel size (D) and the strides (S).\n",
    "\n",
    "$$\n",
    "O = [(I-F)/S]+1 \\times D\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 convolution layers with $3\\times 3$ filters, outputting 100, 200 and 400 feature maps. \n",
    "\n",
    "The first convolution layer outputs images of size $100 \\times 150$. The second convolution layer outputs images of size $50 \\times 75$. The third convolution layer outputs images of size $25 \\times 38$. \n",
    "\n",
    "As an input we take RGB images of resolution $200 \\times 300$ pixels.\n",
    "\n",
    "Lets start by computing the number of parameters for each convolutional layer:\n",
    "- **Convolutional layer 1:** We take an RGB image as input and transform it into 100 new channels. To obtain each of these channels, we will need 3 filters for each channel. Therefore per output channel we will have $3 \\times 3 \\times 3 = 27$ parameters. In total we need to train **2700 parameters** for the first convolutional layer.\n",
    "- **Convolutional layer 2:** Here we take 100 channels as input. Therefore, for each output channel we need $100 \\times 3 \\times 3 = 900$ parameters. Since we have 200 ouput channels, we will need to train **180,000 parameters**.\n",
    "- **Convolutional layer 3:** Here we take 200 channels as input, therefore for each output channel we will need $200 \\times 3 \\times 3 = 1800$ parameters. So in total we will need to train **720,000 parameters**.\n",
    "\n",
    "Summing over all parameters, our network has **902,700 trainable parameters**. Each of these parameters will be represented as an 32-bit float (4 bytes), so all these parameters will take up $3,610,800$ bytes or $3.6$ MB of RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction for a single instnce\n",
    "Now we want to answer how much memory the channels take.\n",
    "- **Convolutional layer 1:** We take a 3D arrat with $3\\times 200 \\times 300$ values. We output 100 channels. With stride 2 the output for each channel (RGB) is $100\\times 150$. These output is summarized into a single output of the same size, so per output channel we need $4 \\times 100 \\times 150$ values. Going through the first convolutional layer will require:\n",
    "\n",
    "$$\n",
    "(3+1)\\times 100 \\times 150 \\times 100 = 6,000,000 \\text{ values}\n",
    "$$\n",
    "\n",
    "- **Convolution layer 2:** Similarly for this layer\n",
    "\n",
    "$$\n",
    "(100+1) \\times 50 \\times 75 \\times 200 = 75,750,000 \\text{ values}\n",
    "$$\n",
    "\n",
    "-- **Convolution layer 3:** Similarly for this layer\n",
    "\n",
    "$$\n",
    "(200+1) \\times 25 \\times 38 \\times 400 = 74,370,000 \\text{ values}\n",
    "$$\n",
    "\n",
    "So in total, if we wanted to fit all values into main memory, we would need:\n",
    "\n",
    "$$\n",
    "(6,000,000 + 75,750,000+74,370,000)\\times 4 /1000 /1000 = 624.5 MB\n",
    "$$\n",
    "\n",
    "We would need approximately **628 MB of memory** to predict a single instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on a mini batch of 50 images\n",
    "Training on a mini batch of 50 images will\n",
    "\n",
    "$$\n",
    "624.5\\times 50 +3.5 \\approx 31.2 \\text{ GB}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max pooling layer\n",
    "Pooling layer help us reduce the dimensions of the output layers by essentially simmarizing them and as such reducing the dimension by several factors.\n",
    "\n",
    "The purpose of pooling layers is to\n",
    "- reduce the computational load\n",
    "- reduce memory usage\n",
    "- reduce the number of parameters (thereby limiting the risk of overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "#### Solving a Fashion_MNIST with LeNet architecture\n",
    "Here you will implement a LeNet architecture of Convolutional Neural Networks (as briefly introduced in the lecture and shown in LeNet.pdf here) with pyTorch (you may get inspired by the code in exampleCNN.ipynb).\n",
    "\n",
    "First you will download the Fashion-MNIST dataset. Split into train/validation test datasets and train the network. Finally, plot the learning curves (train/validation loss and accuracy) and show the confusion matrix.\n",
    " 1. Download Fashion-MNIST\n",
    " 2. Split the data into train / validation / test subsets. Make mini-batches if necesssary.\n",
    " 3. Build a CNN model\n",
    " 4. Train the model on the dataset\n",
    " 5. Plot the training curves (Loss and accuracy)\n",
    " 6. Show the confusion matrix and accuracy on the test dataset.\n",
    " 7. Try adding Droput layers; play with the hyperparameters. Use cross-valudation to find the best hyperparameters\n",
    " 8. When you train the best model, visualize the filters of the first convolutional layer. You may look at an example on how to visualize filters in PyTorch: https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\idawe\\OneDrive\\Dokumenter\\ITU\\third_semester\\MachineLearning\\20_Exercises\\20_Exercises.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/idawe/OneDrive/Dokumenter/ITU/third_semester/MachineLearning/20_Exercises/20_Exercises.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_set, val_set \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mrandom_split(data, [\u001b[39m50000\u001b[39m, \u001b[39m10000\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(data, [50000, 10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make mini batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batches are a way to train an epoch on a smaller random sample of the training data to avoid overfitting and improve the performance with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, **INPUT):\n",
    "\n",
    "        # Inherit from nn module\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # Define activation function\n",
    "        self.af = INPUT.get('af')\n",
    "\n",
    "        # Define drop ratio\n",
    "        self.dpr = INPUT.get('dpr')\n",
    "\n",
    "        # Define neural network architecture\n",
    "        self.nn = nn.Sequential(\n",
    "                # C1 6@28x28\n",
    "                nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
    "                nn.BatchNorm2d(6),\n",
    "                self.af(),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                \n",
    "                # C2: 16@10x10\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(16),\n",
    "                self.af(),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                \n",
    "                # Apply flattening on the output\n",
    "                nn.Flatten(),\n",
    "                \n",
    "                # Dense part\n",
    "                # L1\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Linear(16 * 5 * 5, 120),\n",
    "                nn.BatchNorm1d(120),\n",
    "                self.af(),\n",
    "                \n",
    "                # L2\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Linear(120, 84),\n",
    "                nn.BatchNorm1d(84),\n",
    "                self.af(),\n",
    "                \n",
    "                # L3\n",
    "                nn.Dropout(self.dpr),\n",
    "                nn.Linear(84, 10))\n",
    "        \n",
    "        # Define batch size\n",
    "        self.batch_size = INPUT.get('batch_size')\n",
    "\n",
    "        # Define datasets\n",
    "        self.training_data = DataLoader(\n",
    "            INPUT.get('trd'), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "        self.validation_data = DataLoader(\n",
    "            INPUT.get('vd'), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "        self.test_data = DataLoader(\n",
    "            INPUT.get('ted'), batch_size=self.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss_fn = INPUT.get('loss_fn')\n",
    "\n",
    "        # Define learning rate\n",
    "        self.lr = INPUT.get('lr')\n",
    "\n",
    "        # Define numper of epochs\n",
    "        self.epochs = INPUT.get('epochs')\n",
    "\n",
    "        # Define optimizer\n",
    "        self.optimizer = INPUT.get('optim')(self.parameters(), lr=self.lr)\n",
    "\n",
    "        # Save training progress\n",
    "        self.loss_history = []\n",
    "        self.acc_history = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.nn(x)\n",
    "        return logits\n",
    "    \n",
    "    def train_loop(self):\n",
    "        \n",
    "        size = len(self.training_data.dataset)\n",
    "        for batch, (X, y) in enumerate(self.training_data):\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred = self.forward(X)\n",
    "            loss = self.loss_fn(pred, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    def val_loop(self):\n",
    "        size = len(self.validation_data.dataset)\n",
    "        num_batches = len(self.validation_data)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in self.validation_data:\n",
    "                pred = self.forward(X)\n",
    "                test_loss += self.loss_fn(pred, y).item()\n",
    "                correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        \n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "\n",
    "        print(\n",
    "            f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        # Save it to history\n",
    "        self.acc_history.append(correct)\n",
    "        self.loss_history.append(test_loss)\n",
    "\n",
    "    def visualize(self):\n",
    "        x = [i for i in range(self.epochs)]\n",
    "        y1 = self.acc_history\n",
    "        y2 = self.loss_history\n",
    "        plt.plot(x, y1)\n",
    "        plt.plot(x, y2)\n",
    "        plt.show()\n",
    "\n",
    "    def fit(self):\n",
    "        for t in range(self.epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            self.train_loop()\n",
    "            self.val_loop()\n",
    "        print(\"Done!\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        return softmax(logits).argmax(1)\n",
    "\n",
    "    def test(self):\n",
    "        # Get data\n",
    "        X, y = next(iter(self.test_data))\n",
    "\n",
    "        # Predict values\n",
    "        y_hat = self.predict(X)\n",
    "\n",
    "        print(\"Accuracy score for test data\")\n",
    "        print(\"-\"*60)\n",
    "        print(f\"Acc: {accuracy_score(y, y_hat)*100} %\")\n",
    "        print()\n",
    "        print(\"Confusion matrix for test data\")\n",
    "        print(\"-\"*60)\n",
    "        print(confusion_matrix(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.382711  [    0/50000]\n",
      "loss: 0.812746  [10000/50000]\n",
      "loss: 0.464937  [20000/50000]\n",
      "loss: 0.500483  [30000/50000]\n",
      "loss: 0.361832  [40000/50000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.439631 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.639330  [    0/50000]\n",
      "loss: 0.496878  [10000/50000]\n",
      "loss: 0.476845  [20000/50000]\n",
      "loss: 0.400529  [30000/50000]\n",
      "loss: 0.300162  [40000/50000]\n",
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.383186 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.272244  [    0/50000]\n",
      "loss: 0.332273  [10000/50000]\n",
      "loss: 0.286407  [20000/50000]\n",
      "loss: 0.312728  [30000/50000]\n",
      "loss: 0.198159  [40000/50000]\n",
      "Test Error: \n",
      " Accuracy: 87.6%, Avg loss: 0.336496 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.171026  [    0/50000]\n",
      "loss: 0.361318  [10000/50000]\n",
      "loss: 0.343658  [20000/50000]\n",
      "loss: 0.326028  [30000/50000]\n",
      "loss: 0.266119  [40000/50000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.346077 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.344062  [    0/50000]\n",
      "loss: 0.380922  [10000/50000]\n",
      "loss: 0.453339  [20000/50000]\n",
      "loss: 0.638531  [30000/50000]\n",
      "loss: 0.315774  [40000/50000]\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.320767 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Get training data\n",
    "data = datasets.FashionMNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(), \n",
    "    download = True,            \n",
    ")\n",
    "\n",
    "# Get test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "# Split training into  training and validation\n",
    "g_cpu = torch.Generator()\n",
    "g_cpu.manual_seed(3)\n",
    "training_data, val_data = torch.utils.data.random_split(data, [50000, 10000])\n",
    "\n",
    "# Initialize model\n",
    "INPUT = {\n",
    "    'batch_size': 100,\n",
    "    'trd': training_data,\n",
    "    'vd': val_data,\n",
    "    'ted': test_data,\n",
    "    'loss_fn': nn.CrossEntropyLoss(),\n",
    "    'lr': 1e-1,\n",
    "    'epochs': 5,\n",
    "    'af': nn.Sigmoid,\n",
    "    'optim': torch.optim.Adam,\n",
    "    'dpr': 1e-3\n",
    "\n",
    "}\n",
    "model = CNN(**INPUT)\n",
    "\n",
    "# Train model\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the confusion matrix and accuracy on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimise model\n",
    "Try adding Droput layers; play with the hyperparameters. Use cross-valudation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the filters of the first convolution layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
